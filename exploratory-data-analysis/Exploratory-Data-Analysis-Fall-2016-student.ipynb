{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In this tutorial we focus on two popular methods for exploring high dimensional datasets. \n",
    "\n",
    "1. [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "2. [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "\n",
    "The first method is a general scheme for dimensionality reduction, but the second one is specifically used in the text domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------------------------\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a popular method for summarizing datasets. Suppose, we have a dataset of different [wine](http://archive.ics.uci.edu/ml/datasets/Wine) types. We describe each wine sample by its Alcohol content, color, and so on (see this very nice visualization of wine properties taken from here). Some of these features will measure related properties and so will be redundant. So, we can summarize each wine sample with fewer features! PCA is one such way to do this. It's also called as a method for dimensionality reduction.\n",
    "\n",
    "\n",
    "Here we have a scatter plot of different wine samples (synthetic). It's based on two wine characteristics, color intensity and alcohol content. \n",
    "<img src=\"http://i.stack.imgur.com/jPw90.png\">\n",
    "We notice a correlation between these two features. We can construct a new property or feature (that summarizes the two features) by drawing a line through the *center of the scatter plot* and *projecting* all points onto this line. We construct these lines via linear combinations of $x$ and $y$ coordinates, i.e., $w_1 x + w_2 y$. Each configuration of $(w_1, w_2)$ will give us a new line.\n",
    "\n",
    "Now we will look at the projections -- The below animation shows how the projections of data points look like for different lines (red dots are projections of the blue dots):\n",
    "<img src=\"http://i.stack.imgur.com/Q7HIP.gif\">\n",
    "\n",
    "PCA aims to find **the best line** according to the following two criteria. \n",
    "\n",
    "1. The variation of (projected) values along the line should be maximal. Have look at how the \"variance\" of the red dots changes while the line rotates...\n",
    "\n",
    "2. The line should give the lowest reconstruction error. By reconstruction, we mean that constructing the original two characteristics (the position ($x$, $y$) of a blue dot) from the new one (the position of a red dot). This reconstruction error is proportional to the length of the connecting red line. \n",
    "\n",
    "<img src=\"http://i.stack.imgur.com/XFngC.png\">\n",
    "\n",
    "\n",
    "We will notice that **the maximum variance** and **the minimum error** are happened at the same time, when the line points to the magenta ticks. This line corresponds to the first principal component constructed by PCA.  \n",
    "\n",
    "PCA [objective](http://stats.stackexchange.com/questions/10251): Given the data covariance matrix $C$, we look for a vector $u$ having unit length ($\\|u\\| = 1$) such that $u^TCu$ is maximal. We will see that we can do this with the help of **eigenvectors** and **eigenvalues** of the covariance matrix. \n",
    "\n",
    "We will look at the intuition behind this approach using the example above. \n",
    "\n",
    "Let $C$ be an $n \\times n$ matrix and $u$ is an $n \\times 1$ vector. The operation $C u$ is well-defined. An eigenvector of $C$ is, by definition, any vector $u$ such that $C u = \\lambda u$. For the dataset $A$ ($n \\times 2$ matrix) above, the covariance matrix C ($2 \\times 2$ matrix) is (we assume that the data is centered.)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{vmatrix}\n",
    "1.07 &  0.63 \\\\\n",
    "0.63 &  0.64 \n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "It's a square symmetric matrix. Thus, one can diagonalize it by choosing a new orthogonal coordinate system, given by its eigenvectors ([spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem)): \n",
    "\n",
    "\\begin{equation*}\n",
    "C = U \\Lambda U^{T}\n",
    "\\end{equation*}\n",
    "\n",
    "where $U$ is a matrix of eigenvectors $u_i$'s (each column is an eigenvector) and $\\Lambda$ is a diagonal matrix with eigenvalues $\\lambda_i$'s on the diagonal. \n",
    "\n",
    "In the new (eigen) space, the covariance matrix is diagonal, as follows: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{vmatrix}\n",
    "1.52 &  0 \\\\\n",
    "0 &  0.18 \n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "It means that there is no correlation between points in this new system. The maximum possible variance is $1.52$, which is given by the first eigenvalue. We achieve this variance by taking the projection on the first principal axis. The direction of this axis is given by the first eigen vector of $C$.\n",
    "\n",
    "\n",
    "This example/discussion is adapted from [here](http://stats.stackexchange.com/questions/2691). \n",
    "\n",
    "-------------------------------\n",
    "\n",
    "### PCA on a Real Dataset\n",
    "\n",
    "For illustration, we will use the [wine](http://archive.ics.uci.edu/ml/datasets/Wine) dataset. Each wine sample is described by 14 features as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will first read the wine data headers \n",
    "f = open(\"wine.data\")\n",
    "header = f.readlines()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at two wine characteristics: **Alcohol Content** and **Color Intensity**. \n",
    "\n",
    "<!--img src=\"http://winefolly.com/wp-content/uploads/2013/02/wine-color-chart1.jpg\"-->\n",
    "\n",
    "We can draw a scatter plot:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg as la\n",
    "\n",
    "# Read the data file (text format): wine.data, delimiter=',', use columns 0, 1, 10, skip the header    \n",
    "\n",
    "wine_class, wine_alc, wine_col = np.loadtxt(\"wine.data\", delimiter=',', usecols=(0, 1, 10), unpack=True, skiprows=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw a scatter plot of wine color intensity and alcohol content \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on a Subset of the Wine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform PCA on two wine characteristics: **Alcohol Content** and **Color Intensity**\n",
    "\n",
    "col_alc = np.matrix([wine_col, wine_alc]).T\n",
    "m, n = col_alc.shape\n",
    "\n",
    "# compute column means \n",
    "\n",
    "\n",
    "# center the data with column means \n",
    "\n",
    "\n",
    "# calculate the covariance matrix\n",
    "\n",
    "\n",
    "# calculate eigenvectors & eigenvalues of the covariance matrix\n",
    "\n",
    "\n",
    "# sort eigenvalues and eigenvectors in decreasing order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the normalized data and its principal components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot of the normalized data \n",
    "# color intensity of the x-axis and alcohol content on the y-axis \n",
    "\n",
    "\n",
    "\n",
    "# Plot the principal component line \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the normalized data to the principal component space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the PCA tranformation\n",
    "\n",
    "\n",
    "# Plot the data points in the new space   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework $1$**: Apply PCA on the whole set of features and analyze its principal components.   \n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "## Exploratory Text Analysis\n",
    "\n",
    "First, let's import numpy and a couple other modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We consider a toy document collection (corpus) and a query for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\n",
    "    \"Romeo and Juliet.\", # document 1 \n",
    "    \"Juliet: O happy dagger!\", # document 2\n",
    "    \"Romeo died by dagger.\", # document 3\n",
    "    \"'Live free or die', that's the New-Hampshire's motto.\", # document 4\n",
    "    \"Did you know, New-Hampshire is in New-England.\" # document 5\n",
    "]\n",
    "\n",
    "\n",
    "key_words = [\n",
    "    'die', \n",
    "    'dagger'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We now build a *term frequency* (**TF**) matrix from the corpus using the Python **sklearn** package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the countvetorizer class \n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0, stop_words=None)\n",
    "\n",
    "\n",
    "# transform the corpus based on the count vectorizer \n",
    "\n",
    "\n",
    "\n",
    "# print the vocabulary \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the corpus **vocabulary** terms. \n",
    "\n",
    "Some of these terms are noninformative or [stopwords](http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html), e.g., a, an, the, and, etc. One can use a standard or a custom stopword list to remove these terms.  \n",
    "\n",
    "The vocabulary also contains different forms for a single word, e.g., die, died. One can use methods such are [stemming and lemmatization](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) to get root forms of words in a corpus. \n",
    "\n",
    "There are several open source libraries available to perform all these for you, e.g., [Python Natural Language Processing Toolkit](http://www.nltk.org/) (NLTK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A custom stopword list \n",
    "stop_words = [\"a\", \"an\", \"the\", \"and\", \"in\", \"by\", \"or\", \"did\", \"you\", \"is\", \"that\"]\n",
    "\n",
    "# Here, we assume that we preprocessed the corpus  \n",
    "preprocessed_corpus = [\n",
    "\"Romeo and Juliet\",\n",
    "\"Juliet O happy dagger\",\n",
    "\"Romeo die by dagger\",\n",
    "\"Live free or die that the NewHampshire motto\",\n",
    "\"Did you know NewHampshire is in NewEngland\"\n",
    "]\n",
    "\n",
    "\n",
    "# Customize the vectorizer class \n",
    "\n",
    "\n",
    "# transform the corpus based on the count vectorizer \n",
    "\n",
    "\n",
    "# print the vocabulary \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### TF-IDF\n",
    "\n",
    "Here, we compute the TF-IDF matrix for the normalized corpus and the sample query **die dagger**. We consider the query as a document in the corpus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# query keywords \n",
    "\n",
    "key_words = ['die', 'dagger']\n",
    "\n",
    "# To keep the development simple, we build a composite model for both the corpus and the query \n",
    "\n",
    "corpus = preprocessed_corpus + [' '.join(key_words)]\n",
    "\n",
    "# transform the corpus based on the count vectorizer \n",
    "\n",
    "\n",
    "# TF-IDF transform using TfidfTransformer\n",
    "\n",
    "\n",
    "# transform the TF matrix to TF-IDF matrix \n",
    "\n",
    "# D x V document-term matrix \n",
    "\n",
    "\n",
    "# 1 x V query-term vector \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval via TF-IDF \n",
    "\n",
    "Now, we solve the document ranking problem for the given query: ***die dagger***. We use [cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine) to measure similarity between each document vector and the query vector in the TF-IDF vector space. Once we have the distance scores we can sort them to get a rank list as follows.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find cosine distance b/w the TF-IDF vectors of every document and the query \n",
    "\n",
    "\n",
    "# Sort them and create the rank list \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "We perform LSA using the well-known matrix factorization technique Singular Value Decomposition (SVD). \n",
    "\n",
    "We consider the TF matrix for SVD. In practice, one can also perform SVD on the TF-IDF matrix.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \n",
    "\n",
    "* $A$ is a $V \\times D$ data matrix \n",
    "\n",
    "* $U$ is the matrix of the eigenvectors of $C = AA'$ (the term-term matrix). It's a $V \\times V$ matrix. \n",
    "\n",
    "* $V$ is the matrix of the eigenvectors of $B = A'A$ (the document-document matrix). It's a $D \\times D$ matrix \n",
    "\n",
    "* $s$ is the vector singular values, obtained as square roots of the eigenvalues of $B$.\n",
    "\n",
    "More info can be found in the python SVD documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html\n",
    "\n",
    "We now perform data reduction or transform documents in a $V$-dimensional space to a lower dimensional space. Let's take the number dimensions $K = 3$, i.e., the number of semantic components in the corpus. \n",
    "\n",
    "Using LSA, we can represent vocabulary terms in the semantic space.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 2 # number of components\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval via LSA\n",
    "\n",
    "Now we would like to represent the query in the LSA space. A natural choice is to compute a vector that is the centroid of the semantic vectors for its terms. \n",
    "\n",
    "In our example, the keyword query is **die dagger**. We compute the query vector as        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now solve the document ranking problem given the query **die dagger** as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find cosine distance b/w the TF-IDF vectors of every document and the query \n",
    "\n",
    "\n",
    "# Sort them and create the rank list \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework $2$**: Compare the ranking scores of documents in this list with the ranking scores generated by TF-IDF scheme that we discussed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
